{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DecisionTreeClassifierWithWeight:\n",
    "    def __init__(self):\n",
    "        self.best_err = 1  # 最小的加权错误率\n",
    "        self.best_fea_id = 0  # 最优特征id\n",
    "        self.best_thres = 0  # 选定特征的最优阈值\n",
    "        self.best_op = 1  # 阈值符号，其中 1: >, 0: <\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(len(X)) / len(X)\n",
    "        n = X.shape[1]\n",
    "        for i in range(n):\n",
    "            feature = X[:, i]  # 选定特征列\n",
    "            fea_unique = np.sort(np.unique(feature))  # 将所有特征值从小到大排序\n",
    "            for j in range(len(fea_unique) - 1):\n",
    "                thres = (fea_unique[j] + fea_unique[j + 1]) / 2  # 逐一设定可能阈值\n",
    "                for op in (0, 1):\n",
    "                    y_ = 2 * (feature >= thres) - 1 if op == 1 else 2 * (feature < thres) - 1  # 判断何种符号为最优\n",
    "                    err = np.sum((y_ != y) * sample_weight)\n",
    "                    if err < self.best_err:  # 当前参数组合可以获得更低错误率，更新最优参数\n",
    "                        self.best_err = err\n",
    "                        self.best_op = op\n",
    "                        self.best_fea_id = i\n",
    "                        self.best_thres = thres\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        feature = X[:, self.best_fea_id]\n",
    "        return 2 * (feature >= self.best_thres) - 1 if self.best_op == 1 else 2 * (feature < self.best_thres) - 1\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        y_pre = self.predict(X)\n",
    "        if sample_weight is not None:\n",
    "            return np.sum((y_pre == y) * sample_weight)\n",
    "        return np.mean(y_pre == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier_:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimators = []\n",
    "        self.alphas = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        sample_weight = np.ones(len(X)) / len(X)  # 初始化样本权重为 1/N\n",
    "        for _ in range(self.n_estimators):\n",
    "            dtc = DecisionTreeClassifierWithWeight().fit(X, y, sample_weight)  # 训练弱学习器\n",
    "            alpha = 1 / 2 * np.log((1 - dtc.best_err) / dtc.best_err)  # 权重系数\n",
    "            y_pred = dtc.predict(X)\n",
    "            sample_weight *= np.exp(-alpha * y_pred * y)  # 更新迭代样本权重\n",
    "            sample_weight /= np.sum(sample_weight)  # 样本权重归一化\n",
    "            self.estimators.append(dtc)\n",
    "            self.alphas.append(alpha)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.empty((len(X), self.n_estimators))  # 预测结果二维数组，其中每一列代表一个弱学习器的预测结果\n",
    "        for i in range(self.n_estimators):\n",
    "            y_pred[:, i] = self.estimators[i].predict(X)\n",
    "        y_pred = y_pred * np.array(self.alphas)  # 将预测结果与训练权重乘积作为集成预测结果\n",
    "        return 2 * (np.sum(y_pred, axis=1) > 0) - 1  # 以0为阈值，判断并映射为-1和1\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "y = 2 * y - 1  # 将0/1取值映射为-1/1取值\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972027972027972\n",
      "0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "print(AdaBoostClassifier_().fit(x_train, y_train).score(x_test, y_test))  # 0.986013986013986\n",
    "print(AdaBoostClassifier().fit(x_train, y_train).score(x_test, y_test))  # 0.965034965034965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基尼指数计算\n",
    "def gini_calculate(groups, class_values):\n",
    "    gini = 0.0\n",
    "    D = len(groups[0]) + len(groups[1])\n",
    "    for class_value in class_values:\n",
    "        for group in groups:\n",
    "            size = len(group)\n",
    "            if size == 0:\n",
    "                continue\n",
    "            proportion = [row[-1] for row in group].count(class_value) / float(size)\n",
    "            gini += float(size) / D * (proportion * (1.0 - proportion))\n",
    "    return gini\n",
    "\n",
    "\n",
    "# 找出最优分割特征\n",
    "def get_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
    "    features = list()\n",
    "    while len(features) < n_features:\n",
    "        index = randrange(len(dataset[0]) - 1)  #这里是随机森林第二个核心思想，随机选取特征列进行决策树构造!!\n",
    "        if index not in features:\n",
    "            features.append(index)\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = train_split(index, row[index], dataset)\n",
    "            gini = gini_calculate(groups, class_values)\n",
    "            if gini < b_score:\n",
    "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
    "    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n",
    "\n",
    "\n",
    "# 找到分类最多的标签，作为最终预测标签,这里其实和bagging函数的构造方法类似\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "\n",
    "# 递归分类函数构建\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del (node['groups'])\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    if depth >= max_depth:  #若分类还未结束，则选取数据中分类标签较多的作为结果，防止过拟合\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth + 1)\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth + 1)\n",
    "\n",
    "\n",
    "# 构建决策树函数\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    # 返回最优列和相关的信息\n",
    "    root = get_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "\n",
    "# 决策树预测函数predict构建\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "\n",
    "# bagging套袋法构建，这里是随机森林算法核心思想!!\n",
    "def bagging(trees, row):\n",
    "    predictions = [predict(tree, row) for tree in trees]\n",
    "    return max(set(predictions), key=predictions.count)  #这里选取每个决策树预测最多的那个分类结果作为最终预测结果\n",
    "\n",
    "\n",
    "# 决策树采样函数构建\n",
    "def subsample(dataset, ratio):\n",
    "    sample = list()\n",
    "    n_sample = round(len(dataset) * ratio)\n",
    "    while len(sample) < n_sample:\n",
    "        index = randrange(len(dataset))  # 有放回的随机采样\n",
    "        sample.append(dataset[index])\n",
    "    return sample\n",
    "\n",
    "\n",
    "# 随机森林模型构建\n",
    "def random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n",
    "    trees = list()\n",
    "    for i in range(n_trees):\n",
    "        sample = subsample(train, sample_size)\n",
    "        tree = build_tree(sample, max_depth, min_size, n_features)  # 创建一个决策树\n",
    "        trees.append(tree)\n",
    "    predictions = [bagging(trees, row) for row in test]  # bagging预测test\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# 评估函数构造，用ACC作为评价指标\n",
    "def acc(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
